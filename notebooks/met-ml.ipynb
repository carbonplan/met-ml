{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training\n",
    "\n",
    "This notebook loads the ETL data and trains the LSTM. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mamba install -y numpy==1.19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load_ext lab_black\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path\n",
    "\n",
    "import fsspec\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapper = fsspec.get_mapper(f'gs://carbonplan-scratch/metml/etl/x_train.zarr')\n",
    "x_train = xr.open_zarr(mapper, consolidated=True)['x'].load()\n",
    "\n",
    "mapper = fsspec.get_mapper(f'gs://carbonplan-scratch/metml/etl/x_val.zarr')\n",
    "x_val = xr.open_zarr(mapper, consolidated=True)['x'].load()\n",
    "\n",
    "mapper = fsspec.get_mapper(f'gs://carbonplan-scratch/metml/etl/y_train.zarr')\n",
    "y_train = xr.open_zarr(mapper, consolidated=True)['y'].load()\n",
    "\n",
    "mapper = fsspec.get_mapper(f'gs://carbonplan-scratch/metml/etl/y_val.zarr')\n",
    "y_val = xr.open_zarr(mapper, consolidated=True)['y'].load()\n",
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xdims = dict(zip(x_train.dims, x_train.shape))\n",
    "ydims = dict(zip(y_train.dims, y_train.shape))\n",
    "print(xdims, ydims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = (xdims[\"lookback\"], xdims[\"features\"])\n",
    "print(input_shape)\n",
    "\n",
    "\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras import backend\n",
    "\n",
    "# root mean squared error (rmse) for regression (only for Keras tensors)\n",
    "def rmse(y_true, y_pred):\n",
    "    return backend.sqrt(backend.mean(backend.square(y_pred - y_true), axis=-1))\n",
    "\n",
    "\n",
    "# mean squared error (mse) for regression  (only for Keras tensors)\n",
    "def mse(y_true, y_pred):\n",
    "    return backend.mean(backend.square(y_pred - y_true), axis=-1)\n",
    "\n",
    "\n",
    "# coefficient of determination (R^2) for regression  (only for Keras tensors)\n",
    "def r_square(y_true, y_pred):\n",
    "    SS_res = backend.sum(backend.square(y_true - y_pred))\n",
    "    SS_tot = backend.sum(backend.square(y_true - backend.mean(y_true)))\n",
    "    return 1 - SS_res / (SS_tot + backend.epsilon())\n",
    "\n",
    "\n",
    "def bias(y_true, y_pred):\n",
    "    return backend.mean(y_pred) - backend.mean(y_true)\n",
    "\n",
    "\n",
    "metrics = [rmse, mse, r_square, bias]\n",
    "# metrics = []\n",
    "\n",
    "def make_model_1(var, ydims=None):\n",
    "    # design network\n",
    "    name = f\"1_layer_lstm_{var}\"\n",
    "    model = Sequential(name=name)\n",
    "    model.add(LSTM(20, input_shape=input_shape, use_bias=True))\n",
    "    model.add(Dense(ydims[\"features\"]))\n",
    "    model.compile(loss=\"mean_squared_error\", optimizer=\"adam\", metrics=metrics)\n",
    "    return model\n",
    "\n",
    "\n",
    "def make_model_2(var, ydims=None):\n",
    "    # design network\n",
    "    name = f\"2_layer_lstm_{var}\"\n",
    "    model = Sequential(name=name)\n",
    "    model.add(LSTM(20, input_shape=input_shape, use_bias=True, return_sequences=True))\n",
    "    model.add(LSTM(20))\n",
    "    model.add(Dense(ydims[\"features\"]))\n",
    "    model.compile(loss=\"mean_squared_error\", optimizer=\"adam\", metrics=metrics)\n",
    "    return model\n",
    "\n",
    "\n",
    "def make_model_3(var, ydims=None):\n",
    "    # design network\n",
    "    name = f\"3_layer_lstm_{var}\"\n",
    "    model = Sequential(name=name)\n",
    "    model.add(LSTM(20, input_shape=input_shape, use_bias=True, return_sequences=True))\n",
    "    model.add(LSTM(20, return_sequences=True))\n",
    "    model.add(LSTM(20))\n",
    "    model.add(Dense(ydims[\"features\"]))\n",
    "    model.compile(loss=\"mean_squared_error\", optimizer=\"adam\", metrics=metrics)\n",
    "    return model\n",
    "\n",
    "\n",
    "def make_model_4(var, ydims=None):\n",
    "    # design network\n",
    "    name = f\"3_layer_lstm_wide_{var}\"\n",
    "    model = Sequential(name=name)\n",
    "    model.add(LSTM(40, input_shape=input_shape, use_bias=True, return_sequences=True))\n",
    "    model.add(LSTM(40, return_sequences=True))\n",
    "    model.add(LSTM(40))\n",
    "    model.add(Dense(ydims[\"features\"]))\n",
    "    model.compile(loss=\"mean_squared_error\", optimizer=\"adam\", metrics=metrics)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the model\n",
    "# history = {}\n",
    "# for batch_size in [128, 512, 2048, 8192, 16384]:\n",
    "#     model = make_model_1()\n",
    "#     history[batch_size] = model.fit(x_train.values, y_train.values,\n",
    "#                         validation_data=(x_val.values, y_val.values),\n",
    "#                          batch_size=batch_size, epochs=30,\n",
    "#                          shuffle=True, callbacks=callbacks)\n",
    "\n",
    "# plt.figure(figsize=(12, 12))\n",
    "# # plot training history\n",
    "# for batch, h in history.items():\n",
    "# #     plt.plot(h.history['loss'], label=f'train-{batch}')\n",
    "#     plt.plot(h.history['val_loss'], label=f'test-{batch}')\n",
    "# plt.yscale('log')\n",
    "# plt.xscale('log')\n",
    "# plt.legend()\n",
    "\n",
    "# based on this, I'm using the batch_size of 512 for now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_val[:100].plot.line(x=\"samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "for gpu in gpus:\n",
    "    print(\"Name:\", gpu.name, \"  Type:\", gpu.device_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 512\n",
    "history = {}\n",
    "\n",
    "\n",
    "def make_callbacks(name):\n",
    "    mc = ModelCheckpoint(\n",
    "        f\"best_{name}.h5\",\n",
    "        monitor=\"val_mse\",\n",
    "        mode=\"max\",\n",
    "        verbose=0,\n",
    "        save_best_only=True,\n",
    "    )\n",
    "    es = EarlyStopping(monitor=\"val_loss\", mode=\"min\", verbose=0, patience=20)\n",
    "    return [es, mc]\n",
    "\n",
    "for var in y_train.features.values:\n",
    "    yt = y_train.sel(features=var)\n",
    "    yv = y_val.sel(features=var)\n",
    "    ydims = dict(zip(yt.dims, yt.shape))\n",
    "    if 'features' not in ydims:\n",
    "        ydims['features'] = 1\n",
    "    \n",
    "#     for model in [make_model_1(var, ydims), make_model_2(var, ydims), make_model_3(var, ydims), make_model_4(var, ydims)]:\n",
    "    for model in [make_model_1(var, ydims)]:\n",
    "        model.summary()\n",
    "        history[model.name] = model.fit(\n",
    "            x_train.values,\n",
    "            yt.values,\n",
    "            validation_data=(x_val.values, yv.values,),\n",
    "            batch_size=batch_size,\n",
    "            epochs=500,\n",
    "            shuffle=True,\n",
    "            callbacks=make_callbacks(model.name),\n",
    "        )\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h.history.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = [\n",
    "    #     \"val_loss\",\n",
    "    \"val_rmse\",\n",
    "    \"val_mse\",\n",
    "    \"val_r_square\",\n",
    "    \"val_bias\",\n",
    "]\n",
    "\n",
    "fig, axes = plt.subplots(ncols=len(scores), nrows=4, figsize=(20, 16))\n",
    "for i, var in enumerate(y_train.features.values):\n",
    "    for j, score in enumerate(scores):\n",
    "        # plot training history\n",
    "        plt.sca(axes[i, j])\n",
    "        for model, h in history.items():\n",
    "            if var in model:\n",
    "                plt.plot(h.history[score], label=model)\n",
    "        plt.ylabel(var)\n",
    "        # plt.yscale('log')\n",
    "        # plt.xscale('log')\n",
    "        plt.legend()\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"here\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import dump\n",
    "\n",
    "hdump = {}\n",
    "\n",
    "for k, v in history.items():\n",
    "    hdump[k] = v.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dump(hdump, \"train_history.joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdump = load(\"train_history.joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_df = pd.DataFrame()\n",
    "for var in y_train.features.values:\n",
    "    var_scores = {}\n",
    "    for model, scores in hdump.items():\n",
    "        if var in model:\n",
    "            key = model.replace(\"_\" + var, \"\")\n",
    "            var_scores[key] = max(scores[\"r_square\"])\n",
    "    scores_df[var] = pd.Series(var_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_df.T.plot.bar()\n",
    "plt.legend(loc=\"center left\", bbox_to_anchor=(1.0, 0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_df.plot.bar()\n",
    "plt.legend(loc=\"center left\", bbox_to_anchor=(1.0, 0.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on this analysis, it seems like the 3_layer_lstm_wide model is performing best for all four variables. We'll go with that for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:notebook] *",
   "language": "python",
   "name": "conda-env-notebook-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
