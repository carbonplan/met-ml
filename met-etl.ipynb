{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ETL\n",
    "*Extract-transform-load*\n",
    "\n",
    "This notebook does the data engineering steps required for the Met-ML training and evaluation:\n",
    "\n",
    "- load fluxnet csvs\n",
    "- fit transformers on the full dataset\n",
    "- saves the preprocessed data and transformers for use in the next steps of the project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "\n",
    "from dask.distributed import Client\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, FunctionTransformer, MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from joblib import load, dump\n",
    "\n",
    "from fluxnet_etl import load_fluxnet, make_lookback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the fluxnet dataset\n",
    "reload = False\n",
    "if reload:\n",
    "    # use dask to speed things up\n",
    "    client = Client(n_workers=25)\n",
    "    x_data_computed, y_data_computed, meta = load_fluxnet(compute=True)\n",
    "    \n",
    "    dump(x_data_computed, './etl_data/x_data_computed.joblib')\n",
    "    dump(y_data_computed, './etl_data/y_data_computed.joblib')\n",
    "    dump(meta, './etl_data/meta.joblib')\n",
    "else:\n",
    "    x_data_computed = load('./etl_data/x_data_computed.joblib')\n",
    "    y_data_computed = load('./etl_data/y_data_computed.joblib')\n",
    "    meta = load('./etl_data/meta.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_x_transformers(dfs):\n",
    "    '''takes a list of dataframes, returns a fit transformer'''\n",
    "    # concat all the dataframes together\n",
    "    df = pd.concat(dfs).reset_index(drop=True)\n",
    "    ct = ColumnTransformer(\n",
    "        transformers=[\n",
    "            (\"P\", FunctionTransformer(np.cbrt, validate=False), ['P']),\n",
    "            (\"t_min\", StandardScaler(), ['t_min']),\n",
    "            (\"t_max\", StandardScaler(), ['t_max']),\n",
    "            (\"t\", 'passthrough', ['t']),  # already between 0 and 1\n",
    "            (\"lat\", 'passthrough', ['lat']),  # already between 0 and 1\n",
    "            (\"elev\", StandardScaler(), ['elev'])  # maybe this should be a MinMaxScaler\n",
    "        ],\n",
    "    )\n",
    "    ct.fit(df)\n",
    "    return ct\n",
    "\n",
    "\n",
    "def fit_y_transformers(dfs):\n",
    "    '''takes a list of dataframes, returns a fit transformer'''\n",
    "    # concat all the dataframes together\n",
    "    df = pd.concat(dfs).reset_index(drop=True)\n",
    "    trans = MinMaxScaler()  # scale between 0 and 1\n",
    "    trans.fit(df)\n",
    "    return trans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train (464187, 90, 6)\n",
      "x_val (55000, 90, 6)\n",
      "y_train (464187, 1)\n",
      "y_val (55000, 1)\n"
     ]
    }
   ],
   "source": [
    "def split(x_dfs, y_dfs, test_size=365):\n",
    "    '''split the fluxnet dataset into training and test groups\n",
    "    \n",
    "    This currently just keeps the last N (default=365) days for test samples\n",
    "    \n",
    "    In the future, this should support splitting out complete stations\n",
    "    '''\n",
    "\n",
    "    x_train = []\n",
    "    x_val = []\n",
    "    y_train = []\n",
    "    y_val = []\n",
    "\n",
    "    for x, y in zip(x_dfs, y_dfs):\n",
    "        xt, xv, yt, yv = train_test_split(x, y,\n",
    "                                          test_size=test_size,\n",
    "                                          shuffle=False)\n",
    "        x_train.append(xt)\n",
    "        y_train.append(yt)\n",
    "        x_val.append(xv)\n",
    "        y_val.append(yv)\n",
    "    \n",
    "    return x_train, x_val, y_train, y_val\n",
    "\n",
    "\n",
    "# split the data into train/val groups\n",
    "x_train, x_val, y_train, y_val = split(x_data_computed, y_data_computed)\n",
    "\n",
    "# save x data\n",
    "lookback = 90\n",
    "\n",
    "# fit the x-transformer\n",
    "x_trans = fit_x_transformers(x_data_computed)\n",
    "dump(x_trans, './etl_data/x_trans.joblib')  # save for later\n",
    "\n",
    "# create the 3D tensor for the LSTM including a lookback dimension\n",
    "for name, df_list in zip(['x_train', 'x_val'], [x_train, x_val]):\n",
    "    features = df_list[0].columns\n",
    "    da = xr.concat([make_lookback(x_trans.transform(df), features, lookback=lookback)\n",
    "                    for df in df_list],\n",
    "                   dim='samples')\n",
    "    da.name = name\n",
    "    print(name, da.shape)\n",
    "    da.to_netcdf(f'./etl_data/{name}.nc')  # save for later\n",
    "    \n",
    "\n",
    "# save y data\n",
    "# fit the y-transformer\n",
    "y_trans = fit_y_transformers(y_data_computed)\n",
    "dump(y_trans, './etl_data/y_trans.joblib')  # save for later\n",
    "\n",
    "# transform the targets and save all samples (minus the lookback) for later\n",
    "for name, df_list in zip(['y_train', 'y_val'], [y_train, y_val]):\n",
    "    da = xr.concat([xr.DataArray(y_trans.transform(df[lookback:]),\n",
    "                                 dims=('sample', 'feature'))\n",
    "                    for df in df_list], dim='sample')\n",
    "    da.name = name\n",
    "    print(name, da.shape)   \n",
    "    da.to_netcdf(f'./etl_data/{name}.nc')  # save for later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:projects-met-ml]",
   "language": "python",
   "name": "conda-env-projects-met-ml-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
